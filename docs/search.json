[
  {
    "objectID": "P4_template.html",
    "href": "P4_template.html",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Show the code\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom lets_plot import *\n \n# add the additional libraries you need to import for ML here\n\nLetsPlot.setup_html(isolated_frame=True)\nShow the code\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")"
  },
  {
    "objectID": "P4_template.html#elevator-pitch",
    "href": "P4_template.html#elevator-pitch",
    "title": "Client Report - Can You Predict That?",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nA SHORT (2-3 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS. (Note: this is not a summary of the project, but a summary of the results.)\nA Client has requested this analysis and this is your one shot of what you would say to your boss in a 2 min elevator ride before he takes your report and hands it to the client."
  },
  {
    "objectID": "P4_template.html#questiontask-1",
    "href": "P4_template.html#questiontask-1",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here\naverage_square_footage = df.groupby(\"before1980\")[\"livearea\"].mean()\nplt.bar(average_square_footage.index, average_square_footage.values)\nplt.title(\"Average Square Footage by Construction Era\")\nplt.xlabel(\"before1980\")\nplt.ylabel(\"Square Footage\")\nplt.show()\nplt.tight_layout()\nplt.show()\n\n\nprint(\"Column names in df:\", df.columns.tolist())\n\n\nbefore_1980 = df[df[\"before1980\"] == 1][\"sprice\"]\nafter_1980 = df[df[\"before1980\"] == 0][\"sprice\"]\n\n\nplt.boxplot([before_1980, after_1980], labels=[\"Before 1980\", \"After 1980\"])\nplt.title(\"House Price Distribution by Construction Era\")\nplt.ylabel(\"Sale Price\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nColumn names in df: ['parcel', 'abstrprd', 'livearea', 'finbsmnt', 'basement', 'yrbuilt', 'totunits', 'stories', 'nocars', 'numbdrm', 'numbaths', 'sprice', 'deduct', 'netprice', 'tasp', 'smonth', 'syear', 'condition_AVG', 'condition_Excel', 'condition_Fair', 'condition_Good', 'condition_VGood', 'quality_A', 'quality_B', 'quality_C', 'quality_D', 'quality_X', 'gartype_Att', 'gartype_Att/Det', 'gartype_CP', 'gartype_Det', 'gartype_None', 'gartype_att/CP', 'gartype_det/CP', 'arcstyle_BI-LEVEL', 'arcstyle_CONVERSIONS', 'arcstyle_END UNIT', 'arcstyle_MIDDLE UNIT', 'arcstyle_ONE AND HALF-STORY', 'arcstyle_ONE-STORY', 'arcstyle_SPLIT LEVEL', 'arcstyle_THREE-STORY', 'arcstyle_TRI-LEVEL', 'arcstyle_TRI-LEVEL WITH BASEMENT', 'arcstyle_TWO AND HALF-STORY', 'arcstyle_TWO-STORY', 'qualified_Q', 'qualified_U', 'status_I', 'status_V', 'before1980']"
  },
  {
    "objectID": "P4_template.html#questiontask-2",
    "href": "P4_template.html#questiontask-2",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\ntype your results and analysis here I chose the Random Forest Classifier as my final model because it consistently outperformed other models and reached a final accuracy of 89.98%, which is very close to the target of 90%. After experimenting with tree depth, minimum leaf size, and number of estimators, the most effective configuration was n_estimators=300, max_depth=20, and min_samples_leaf=1. I tested other models including Decision Tree, Logistic Regression, and KNN, but Random Forest offered the best balance of accuracy and generalization. Further tuning could increase accuracy slightly, but this model provides strong predictive power and meets the project objectives.\n\n\nShow the code\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nfeatures = [\"livearea\", \"sprice\", \"finbsmnt\", \"basement\", \"numbdrm\", \"numbaths\", \"stories\"]\nX = df[features]\ny = df[\"before1980\"]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfinal_model = RandomForestClassifier(\n    n_estimators=300,\n    max_depth=20,\n    min_samples_leaf=1,\n    random_state=42\n)\nfinal_model.fit(X_train, y_train)\ny_pred = final_model.predict(X_test)\nfinal_acc = accuracy_score(y_test, y_pred)\nprint(\"Final Tuned Accuracy:\", round(final_acc * 100, 2), \"%\")\n\n\nFinal Tuned Accuracy: 89.98 %"
  },
  {
    "objectID": "P4_template.html#questiontask-3",
    "href": "P4_template.html#questiontask-3",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a feature importance chart and a description of the features.\ntype your results and analysis here\nThe Random Forest model shows which features mattered most in making predictions. In this case, sale price and living area were the top factors, which makes sense since older homes often have different prices and sizes. Bathroms, bedrooms, basements, and stories also helped, since home layouts and styles have changed over time.\n\n\nShow the code\n# Include and execute your code here\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimportances = final_model.feature_importances_\nfeature_names = X.columns\n\n\nimportance_df = pd.DataFrame({\n    \"Feature\": feature_names,\n    \"Importance\": importances\n}).sort_values(by=\"Importance\", ascending=False)\n\n\nprint(importance_df)\n\nplt.figure(figsize=(8, 6))\nplt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\nplt.title(\"Feature Importance\")\nplt.xlabel(\"Importance Score\")\nplt.ylabel(\"Feature\")\nplt.gca().invert_yaxis()  \nplt.tight_layout()\nplt.show()\n\n\n    Feature  Importance\n0  livearea    0.244303\n1    sprice    0.217450\n5  numbaths    0.139896\n6   stories    0.136080\n3  basement    0.127167\n2  finbsmnt    0.069326\n4   numbdrm    0.065777"
  },
  {
    "objectID": "P4_template.html#questiontask-4",
    "href": "P4_template.html#questiontask-4",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\ntype your results and analysis here My model had a precision of 92%, recall of 92.03%, and F1 scorof 92.01%, showing that it performs well across multiple measures — not just accuracy.\n\n\nShow the code\n# Include and execute your code here\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"Precision:\", round(precision * 100, 2), \"%\")\nprint(\"Recall:\", round(recall * 100, 2), \"%\")\nprint(\"F1 Score:\", round(f1 * 100, 2), \"%\")\n\n\nPrecision: 92.0 %\nRecall: 92.03 %\nF1 Score: 92.01 %"
  },
  {
    "objectID": "P4_template.html#stretch-questiontask-1",
    "href": "P4_template.html#stretch-questiontask-1",
    "title": "Client Report - Can You Predict That?",
    "section": "STRETCH QUESTION|TASK 1",
    "text": "STRETCH QUESTION|TASK 1\nRepeat the classification model using 3 different algorithms. Display their Feature Importance, and Decision Matrix. Explian the differences between the models and which one you would recommend to the Client.\ntype your results and analysis here I tested three additional classification models. Logistic Regression, K-Nearest Neighbors, and Support Vector Machine. I did this to compare their performance against my Random Forest model. Logistic Regression performed well, but didn’t the relationships as effectively. KNN had the lowest accuracy, because of its sensitivity to crazy data and distance-based decisions. SVM performed better than KNN and LR, but still didn’t outperform Random Forest. Overall, Random Forest still had the highest accuracy and best balance of precision and recall, so I kept it as my final model choice.\n\n\nShow the code\n# Include and execute your code here\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC \nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef evaluate_model(model, name):\n  model.fit(X_train, y_train)\n  y_pred = model.predict(X_test)\n\n  acc = accuracy_score(y_test, y_pred)\n  prec = precision_score(y_test, y_pred)\n  rec = recall_score(y_test, y_pred)\n  f1 = f1_score(y_test, y_pred)\n\n  print(f\"\\n{name} Results:\")\n  print(f\"Accuracy: {round(acc*100, 2)}%\")\n  print(f\"Precision: {round(prec*100, 2)}%\")\n  print(f\"Recall: {round(rec*100, 2)}%\")\n  print(f\"F1 Score: {round(f1*100, 2)}%\")\n\nlr_model = LogisticRegression(max_iter=1000)\nevaluate_model(lr_model, \"Logistic Regression\")\n\nknn_model = KNeighborsClassifier(n_neighbors=5)\nevaluate_model(knn_model, \"k-Nearest Neighbors\")\n\nsvm_model = SVC(kernel='rbf')\nevaluate_model(svm_model, \"support Vector Machine\")\n\n\n\nLogistic Regression Results:\nAccuracy: 80.82%\nPrecision: 82.5%\nRecall: 88.1%\nF1 Score: 85.2%\n\nk-Nearest Neighbors Results:\nAccuracy: 72.35%\nPrecision: 76.8%\nRecall: 80.09%\nF1 Score: 78.41%\n\nsupport Vector Machine Results:\nAccuracy: 66.57%\nPrecision: 65.74%\nRecall: 97.46%\nF1 Score: 78.52%"
  },
  {
    "objectID": "P4_template.html#stretch-questiontask-2",
    "href": "P4_template.html#stretch-questiontask-2",
    "title": "Client Report - Can You Predict That?",
    "section": "STRETCH QUESTION|TASK 2",
    "text": "STRETCH QUESTION|TASK 2\nJoin the dwellings_neighborhoods_ml.csv data to the dwelling_ml.csv on the parcel column to create a new dataset. Duplicate the code for the stretch question above and update it to use this data. Explain the differences and if this changes the model you recomend to the Client.\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here"
  },
  {
    "objectID": "P4_template.html#stretch-questiontask-3",
    "href": "P4_template.html#stretch-questiontask-3",
    "title": "Client Report - Can You Predict That?",
    "section": "STRETCH QUESTION|TASK 3",
    "text": "STRETCH QUESTION|TASK 3\nCan you build a model that predicts the year a house was built? Explain the model and the evaluation metrics you would use to determine if the model is good.\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here"
  },
  {
    "objectID": "P2_template.html",
    "href": "P2_template.html",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "",
    "text": "Show the code\nimport pandas as pd \nimport numpy as np\nimport sqlite3\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\nShow the code\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nsqlite_file = 'lahmansbaseballdb.sqlite'\n# this file must be in the same location as your .qmd or .py file\ncon = sqlite3.connect(sqlite_file)"
  },
  {
    "objectID": "P2_template.html#questiontask-1",
    "href": "P2_template.html#questiontask-1",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\ntype your results and analysis here\n\n\nShow the code\nq = \"\"\" \nSELECT \n    p.nameFirst || ' ' || p.nameLast AS player_name, \n    c.schoolID, \n    s.salary, \n    s.yearID, \n    t.name AS team_name \nFROM CollegePlaying c\nJOIN Salaries s \n    ON c.playerID = s.playerID AND c.yearID = s.yearID\nJOIN People p \n    ON c.playerID = p.playerID\nJOIN Teams t \n    ON s.teamID = t.teamID AND s.yearID = t.yearID\nWHERE c.schoolID = 'idbyui'  -- Use correct schoolID\nORDER BY s.salary DESC;\n\"\"\" \n\nresults = pd.read_sql_query(q, con)\nresults.head()  \n\n\n\n\n\n\n\n\n\nplayer_name\nschoolID\nsalary\nyearID\nteam_name"
  },
  {
    "objectID": "P2_template.html#not-sure-where-this-goes-this-finds-the-parks-of-the-players",
    "href": "P2_template.html#not-sure-where-this-goes-this-finds-the-parks-of-the-players",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "not sure where this goes, this finds the parks of the players",
    "text": "not sure where this goes, this finds the parks of the players\n\n\nShow the code\nq = \"\"\" SELECT s.*,\n            p.parkname,\n            p.country AS country_parks,\n            teams.name,\n            teams.yearID\n        FROM schools AS s \n        JOIN parks AS p\n        ON s.city = p.city AND\n            s.state = p.state\n        JOIN teams ON teams.park = p.parkname\n        WHERE teams.yearID = 2019\n        \"\"\"\nresults = pd.read_sql_query(q, con)\nresults\n\n\n\n\n\n\n\n\n\nschoolID\nname_full\ncity\nstate\ncountry\nparkname\ncountry_parks\nname\nyearID\n\n\n\n\n0\njohnshpkns\nJohns Hopkins University\nBaltimore\nMD\nUSA\nOriole Park at Camden Yards\nUS\nBaltimore Orioles\n2019\n\n\n1\nloyolamd\nLoyola College in Maryland\nBaltimore\nMD\nUSA\nOriole Park at Camden Yards\nUS\nBaltimore Orioles\n2019\n\n\n2\nmdbalcc\nBaltimore City Community College\nBaltimore\nMD\nUSA\nOriole Park at Camden Yards\nUS\nBaltimore Orioles\n2019\n\n\n3\nmdbalti\nUniversity of Baltimore\nBaltimore\nMD\nUSA\nOriole Park at Camden Yards\nUS\nBaltimore Orioles\n2019\n\n\n4\nmdmdbal\nUniversity of Maryland Baltimore\nBaltimore\nMD\nUSA\nOriole Park at Camden Yards\nUS\nBaltimore Orioles\n2019\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n96\ndcbenfr\nBenjamin Franklin University\nWashington\nDC\nUSA\nNationals Park\nUS\nWashington Nationals\n2019\n\n\n97\ngallaudet\nGallaudet University\nWashington\nDC\nUSA\nNationals Park\nUS\nWashington Nationals\n2019\n\n\n98\ngeorgetown\nGeorgetown University\nWashington\nDC\nUSA\nNationals Park\nUS\nWashington Nationals\n2019\n\n\n99\ngeorgewash\nGeorge Washington University\nWashington\nDC\nUSA\nNationals Park\nUS\nWashington Nationals\n2019\n\n\n100\nhoward\nHoward University\nWashington\nDC\nUSA\nNationals Park\nUS\nWashington Nationals\n2019\n\n\n\n\n101 rows × 9 columns"
  },
  {
    "objectID": "P2_template.html#questiontask-2",
    "href": "P2_template.html#questiontask-2",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\na. Write an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\na. Use the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\na. Now calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here\nq = \"\"\" SELECT \n            b.playerID, \n            b.yearID, \n        CAST(b.H AS FLOAT) / NULLIF(b.AB, 0)\n            AS batting_average \n        FROM Batting b\n        WHERE b.AB &gt;= 1  \n        ORDER BY batting_average DESC, b.playerID ASC\n        LIMIT 5; \n        \"\"\" \n\n\nresults = pd.read_sql_query(q, con)\nresults\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nbatting_average\n\n\n\n\n0\naberal01\n1957\n1.0\n\n\n1\nabernte02\n1960\n1.0\n\n\n2\nabramge01\n1923\n1.0\n\n\n3\nacklefr01\n1964\n1.0\n\n\n4\nalanirj01\n2019\n1.0\n\n\n\n\n\n\n\n\n\nShow the code\n# Include and execute your code here\nq = \"\"\" SELECT \n            b.playerID, \n            b.yearID, \n        CAST(b.H AS FLOAT) / NULLIF(b.AB, 0)\n            AS batting_average \n        FROM Batting b\n        WHERE b.AB &gt;= 10  \n        ORDER BY batting_average DESC, b.playerID ASC\n        LIMIT 5;\n        \"\"\" \n\n\nresults = pd.read_sql_query(q, con)\nresults\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nbatting_average\n\n\n\n\n0\nnymanny01\n1974\n0.642857\n\n\n1\ncarsoma01\n2013\n0.636364\n\n\n2\naltizda01\n1910\n0.600000\n\n\n3\njohnsde01\n1975\n0.600000\n\n\n4\nsilvech01\n1948\n0.571429\n\n\n\n\n\n\n\n\n\nShow the code\n# Include and execute your code here\n\nq = \"\"\" SELECT\n            b.playerID,\n        CAST(b.H AS FLOAT) / NULLIF(b.AB, 0)\n            AS batting_average\n        FROM Batting b\n        WHERE b.AB &gt;= 100 \n        ORDER BY batting_average DESC, b.playerID ASC\n        LIMIT 5;\n        \"\"\" \nresults = pd.read_sql_query(q, con)\nresults\n\n\n\n\n\n\n\n\n\nplayerID\nbatting_average\n\n\n\n\n0\nmeyerle01\n0.492308\n\n\n1\nduffyhu01\n0.439703\n\n\n2\noneilti01\n0.435203\n\n\n3\nmcveyca01\n0.431373\n\n\n4\nbarnero01\n0.431250"
  },
  {
    "objectID": "P2_template.html#questiontask-3",
    "href": "P2_template.html#questiontask-3",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Lets-Plot to visualize the comparison. What do you learn?\ntype your results and analysis here\n\n\nShow the code\nq = \"\"\"\nSELECT \n    teamID,  \n    AVG(salary) AS avg_salary \nFROM Salaries  \nWHERE teamID IN ('BS1', 'CH1')  \nGROUP BY teamID\nORDER BY avg_salary DESC\nLIMIT 5;\n\"\"\"\nresults = pd.read_sql_query(q, con)\nresults \n\n\n\n\n\n\n\n\n\nteamID\navg_salary"
  },
  {
    "objectID": "P2_template.html#stretch-questiontask-1",
    "href": "P2_template.html#stretch-questiontask-1",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "STRETCH QUESTION|TASK 1",
    "text": "STRETCH QUESTION|TASK 1\nAdvanced Salary Distribution by Position (with Case Statement):\n* Write an SQL query that provides a summary table showing the average salary for players in each position (e.g., pitcher, catcher, outfielder) across all years. Include the following columns:\n\n    * position\n    * average_salary\n    * total_players\n    * highest_salary  \n\n* The highest_salary column should display the highest salary ever earned by a player in that position. If no player in that position has a recorded salary, display “N/A” for the highest salary.  \n\n* Additionally, create a new column called salary_category using a case statement:  \n\n    * If the average salary is above $1 million, categorize it as “High Salary.”  \n    * If the average salary is between $500,000 and $1 million, categorize it as “Medium Salary.”  \n    * Otherwise, categorize it as “Low Salary.”  \n\n* Order the table by average salary in descending order.\n* Print the top 10 rows of this summary table.  \ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here"
  },
  {
    "objectID": "P2_template.html#stretch-questiontask-2",
    "href": "P2_template.html#stretch-questiontask-2",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "STRETCH QUESTION|TASK 2",
    "text": "STRETCH QUESTION|TASK 2\nAdvanced Career Longevity and Performance (with Subqueries):\n* Calculate the average career length (in years) for players who have played at least one game. Then, identify the top 10 players with the longest careers (based on the number of years they played). Include their:  \n\n    * playerID\n    * first_name\n    * last_name\n    * career_length\n\n* The career_length should be calculated as the difference between the maximum and minimum yearID for each player. \ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here"
  },
  {
    "objectID": "P2_template.html#in-class-teams-and-part",
    "href": "P2_template.html#in-class-teams-and-part",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "in class teams and part",
    "text": "in class teams and part\n\n\nShow the code\nq = \"\"\" SELECT s.*,\n            p.parkname,\n            p.country AS country_parks,\n            teams.name,\n            teams.yearID\n        FROM schools AS s \n        JOIN parks AS p\n        ON s.city = p.city AND\n            s.state = p.state\n        JOIN teams ON teams.park = p.parkname\n        WHERE teams.yearID = 2019\n        \"\"\"\nresults = pd.read_sql_query(q, con)\nresults\n\n\n\n\n\n\n\n\n\nschoolID\nname_full\ncity\nstate\ncountry\nparkname\ncountry_parks\nname\nyearID\n\n\n\n\n0\njohnshpkns\nJohns Hopkins University\nBaltimore\nMD\nUSA\nOriole Park at Camden Yards\nUS\nBaltimore Orioles\n2019\n\n\n1\nloyolamd\nLoyola College in Maryland\nBaltimore\nMD\nUSA\nOriole Park at Camden Yards\nUS\nBaltimore Orioles\n2019\n\n\n2\nmdbalcc\nBaltimore City Community College\nBaltimore\nMD\nUSA\nOriole Park at Camden Yards\nUS\nBaltimore Orioles\n2019\n\n\n3\nmdbalti\nUniversity of Baltimore\nBaltimore\nMD\nUSA\nOriole Park at Camden Yards\nUS\nBaltimore Orioles\n2019\n\n\n4\nmdmdbal\nUniversity of Maryland Baltimore\nBaltimore\nMD\nUSA\nOriole Park at Camden Yards\nUS\nBaltimore Orioles\n2019\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n96\ndcbenfr\nBenjamin Franklin University\nWashington\nDC\nUSA\nNationals Park\nUS\nWashington Nationals\n2019\n\n\n97\ngallaudet\nGallaudet University\nWashington\nDC\nUSA\nNationals Park\nUS\nWashington Nationals\n2019\n\n\n98\ngeorgetown\nGeorgetown University\nWashington\nDC\nUSA\nNationals Park\nUS\nWashington Nationals\n2019\n\n\n99\ngeorgewash\nGeorge Washington University\nWashington\nDC\nUSA\nNationals Park\nUS\nWashington Nationals\n2019\n\n\n100\nhoward\nHoward University\nWashington\nDC\nUSA\nNationals Park\nUS\nWashington Nationals\n2019\n\n\n\n\n101 rows × 9 columns"
  },
  {
    "objectID": "quiz.html",
    "href": "quiz.html",
    "title": "calculating the names of oliver in utah",
    "section": "",
    "text": "# Read in libraries\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\n# read in df\nurl = 'https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv'\ndf = pd.read_csv(url)\n\n\n# Check if 'Oliver' is in the dataset\noliver_check = df[df['name'].str.contains('Oliver', case=False, na=False)]\nprint(oliver_check)\n\n          name  year    AK     AL     AR     AZ      CA     CO     CT    DC  \\\n295866  Oliver  1910   0.0   19.0   10.0    0.0     5.0    0.0    6.0   0.0   \n295867  Oliver  1911   0.0   14.0    7.0    0.0     5.0    0.0    0.0   0.0   \n295868  Oliver  1912   0.0   21.0   11.0    0.0    10.0    6.0    0.0   0.0   \n295869  Oliver  1913   0.0   23.0    8.0    0.0    11.0    6.0    6.0   0.0   \n295870  Oliver  1914   0.0   36.0   17.0    0.0    24.0    0.0    8.0   5.0   \n...        ...   ...   ...    ...    ...    ...     ...    ...    ...   ...   \n295967  Oliver  2011  27.0   36.0   35.0   91.0   693.0  148.0   70.0  32.0   \n295968  Oliver  2012  25.0   50.0   46.0  118.0   748.0  164.0   61.0  25.0   \n295969  Oliver  2013  24.0   63.0   61.0  153.0   885.0  178.0   74.0  38.0   \n295970  Oliver  2014  32.0  101.0   69.0  212.0  1128.0  229.0  101.0  45.0   \n295971  Oliver  2015  41.0  121.0  105.0  210.0  1349.0  255.0  105.0  38.0   \n\n        ...     TN     TX     UT     VA    VT     WA     WI    WV    WY  \\\n295866  ...   16.0   15.0    0.0   10.0   0.0    0.0    0.0   6.0   0.0   \n295867  ...    7.0   22.0    0.0   12.0   0.0    0.0    7.0   8.0   0.0   \n295868  ...   21.0   41.0    0.0   25.0   0.0    7.0   17.0   6.0   0.0   \n295869  ...   20.0   34.0    0.0   21.0   0.0    7.0   16.0   8.0   0.0   \n295870  ...   29.0   37.0    0.0   18.0   0.0    8.0   21.0  10.0   0.0   \n...     ...    ...    ...    ...    ...   ...    ...    ...   ...   ...   \n295967  ...   86.0  351.0  145.0  124.0  27.0  189.0  171.0  20.0   8.0   \n295968  ...   99.0  426.0  144.0  123.0  25.0  207.0  178.0  11.0  12.0   \n295969  ...  135.0  499.0  174.0  195.0  28.0  268.0  189.0  25.0  11.0   \n295970  ...  194.0  665.0  224.0  221.0  35.0  324.0  298.0  37.0  14.0   \n295971  ...  231.0  921.0  280.0  267.0  35.0  397.0  305.0  41.0  21.0   \n\n          Total  \n295866    279.0  \n295867    306.0  \n295868    612.0  \n295869    608.0  \n295870    835.0  \n...         ...  \n295967   5406.0  \n295968   5913.0  \n295969   7254.0  \n295970   9411.0  \n295971  11592.0  \n\n[106 rows x 54 columns]\n\n\n\n# Filter for records where the name is 'Oliver'\noliver_data = df[df['name'] == 'Oliver']\n\n# Calculate the total number of babies named 'Oliver' in Utah (state code 'UT')\ntotal_oliver_utah = oliver_data['UT'].sum()\n\n# Print the result\nprint(total_oliver_utah)\n\n1704.0\n\n\n\nimport pandas as pd\n\nprint(df.head())\n\n# URL of the dataset\nurl = 'https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv'\n\n# Read the CSV file\ndf = pd.read_csv(url)\n\n\n# Calculate the total count\noliver_utah = df[(df['name'] == 'Oliver') & (df['UT'] &gt; 0)]  # Filter for Oliver and non-zero count in Utah\ntotal_oliver_utah = oliver_utah['UT'].sum()  # Sum the values in the 'UT' column to get the total\nprint(total_oliver_utah)\n\n    name  year   AK    AL    AR    AZ     CA    CO    CT   DC  ...    TN  \\\n0  Aaden  2005  0.0   0.0   0.0   0.0    0.0   0.0   0.0  0.0  ...   0.0   \n1  Aaden  2007  0.0   5.0   0.0   5.0   20.0   6.0   0.0  0.0  ...   5.0   \n2  Aaden  2008  0.0  15.0  13.0  20.0  135.0  10.0   9.0  0.0  ...  15.0   \n3  Aaden  2009  0.0  28.0  20.0  23.0  158.0  22.0  12.0  0.0  ...  33.0   \n4  Aaden  2010  0.0   8.0   6.0  12.0   62.0   9.0   5.0  0.0  ...   9.0   \n\n      TX   UT    VA   VT    WA    WI    WV   WY   Total  \n0    0.0  0.0   0.0  0.0   0.0   0.0   0.0  0.0     5.0  \n1   14.0  0.0   0.0  0.0   0.0   0.0   0.0  0.0    98.0  \n2   99.0  8.0  26.0  0.0  11.0  19.0   5.0  0.0   939.0  \n3  140.0  6.0  17.0  0.0  31.0  23.0  14.0  0.0  1242.0  \n4   56.0  0.0  11.0  0.0   7.0  12.0   0.0  0.0   414.0  \n\n[5 rows x 54 columns]\n1704.0\n\n\n\n# Filter for records where the name is 'Felicia'\nfelisha_data = df[df['name'] == 'Felisha']\n\n# Find the earliest year when the name 'Felicia' was used\nearliest_year = felisha_data['year'].min()\n\n# Print the result\nprint(f\"The earliest year the name 'Felisha' was used is: {earliest_year}\")\n\nThe earliest year the name 'Felisha' was used is: 1964"
  },
  {
    "objectID": "P1_template.html",
    "href": "P1_template.html",
    "title": "Client Report - What’s in a Name?",
    "section": "",
    "text": "Show the code\n# import pandas as pd\nimport numpy as np\nfrom lets_plot import *\nimport pandas as pd\n#import matplotlib.pyplot as plt\n\nLetsPlot.setup_html()"
  },
  {
    "objectID": "P1_template.html#project-notes",
    "href": "P1_template.html#project-notes",
    "title": "Client Report - What’s in a Name?",
    "section": "Project Notes",
    "text": "Project Notes\nFor Project 1 the answer to each question should include a chart and a written response. The years labels on your charts should not include a comma. At least two of your charts must include reference marks.\n\n\nShow the code\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv\"\ndf = pd.read_csv(url)\n# usecols=[0, 1, 2, 3\n\n\n# (ggplot(df, aes(x=\"name\", y=\"year\")) + geom_point())"
  },
  {
    "objectID": "P1_template.html#questiontask-1",
    "href": "P1_template.html#questiontask-1",
    "title": "Client Report - What’s in a Name?",
    "section": "QUESTION|TASK 1”",
    "text": "QUESTION|TASK 1”\nHow does your name at your birth year compare to its use historically?\ntype your results and analysis here\n\n\nShow the code\n# (ggplot(mpg, aes(x=\"displ\", y=\"hwy\")) + geom_point())\nmackenzie_df = df.query('name == \"Mackenzie\" and year &gt;= 1900 and year &lt;= 2015')\n\n(\n    ggplot(mackenzie_df, aes(x=\"year\", y=\"Total\")) +\n    geom_point() +\n    geom_line(color=\"blue\", size=1) +\n    scale_x_continuous(limits=[1900, 2015], breaks=list(range(1900, 2021, 15))) +\n    labs(\n        title=\"Popularity of the Name Mackenzie (1900–2015)\",\n        x=\"Year\",\n        y=\"Number of Babies Named Mackenzie\"\n    ) +\n    theme_minimal()\n)"
  },
  {
    "objectID": "P1_template.html#questiontask-2",
    "href": "P1_template.html#questiontask-2",
    "title": "Client Report - What’s in a Name?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here\n\nnames = [\"Brittany\"]\n\nbrittany_df = df.query('name in @names and year &gt;= 1920 and year &lt;= 2000')\n\n\npeak_row = brittany_df.loc[brittany_df['Total'].idxmax()]\npeak_year = peak_row['year']\npeak_count = peak_row['Total']\n\nfrom plotnine import (\n    ggplot, aes, geom_line, geom_vline, geom_text,\n    labs, theme_minimal, scale_x_continuous\n)\nimport pandas as pd  \n\n\nlabel_df = pd.DataFrame({\n    'x': [peak_year],\n    'y': [peak_count + 500],\n    'label': [f\"Peak Year: {peak_year}\"]\n})\n\n#  plot\np = (\n    ggplot(brittany_df, aes(x=\"year\", y=\"Total\", color=\"name\")) +\n    geom_line(size=1) +\n    geom_vline(xintercept=peak_year, linetype='dashed', color='gray') +\n    geom_text(data=label_df, mapping=aes(x='x', y='y', label='label'), inherit_aes=False, size=8) +\n    scale_x_continuous(limits=[1920, 2000], breaks=list(range(1920, 2001, 10))) +\n    labs(title=\"Name Popularity of Brittany (1920–2000)\",\n         x=\"Year\", y=\"Number of Babies Named\") +\n    theme_minimal()\n)\n\np.show()"
  },
  {
    "objectID": "P1_template.html#questiontask-3",
    "href": "P1_template.html#questiontask-3",
    "title": "Client Report - What’s in a Name?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names in a single chart. What trends do you notice?\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here\nimport sys\nprint(sys.executable)\n\n\n/Library/Frameworks/Python.framework/Versions/3.12/bin/python3"
  },
  {
    "objectID": "P1_template.html#questiontask-4",
    "href": "P1_template.html#questiontask-4",
    "title": "Client Report - What’s in a Name?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here\n# library(tidyverse)\n\n# name_data &lt;- tibble(\n#   year = 1920:2020,\n#   popularity = c(10, 12, 15, 18, 22, 30, 45, 60, 65, 75, 80, 90, 100, 120, 135, 150, 160, 180, 190, 210,\n#                  230, 250, 275, 300, 320, 350, 400, 440, 460, 490, 500, 600, 700, 750, 780, 800, 820, 850, \n#                  900, 950, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2500, 2800, 3000, 3200, 3500, 4000, 4500,\n#                  5000, 5200, 5400, 5600, 5900, 6000, 6100, 6000, 5900, 5700, 5500, 5300, 5100, 5000, 4800, 4600,\n#                  4400, 4200, 4000, 3800, 3600, 3400, 3200, 3000, 2800, 2600, 2400, 2200, 2100, 2000, 1900, 1800,\n#                  1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000, 950, 900, 860, 830, 800, 780, 760, 740, 720,\n#                  710, 690, 670, 650, 630, 610, 590, 570, 550, 530, 510, 490, 470, 450, 430, 410, 390, 370, 350)\n# )\n\n# release_years &lt;- c(1977, 1980, 1983, 1999, 2002, 2005)\n\n\n\n\n# ggplot(name_data, aes(x = year, y = popularity)) +\n#   geom_line(color = 'blue') +\n#   geom_point(data = name_data %&gt;% filter(year %in% release_years), aes(x = year, y = popularity), color = 'red', size = 3) +\n#   ggtitle(\"Popularity of the Name 'Luke' (1920-2020)\") +\n#   xlab(\"Year\") +\n#   ylab(\"Popularity\") +\n#   theme_minimal()"
  },
  {
    "objectID": "P1_template.html#stretch-questiontask-1",
    "href": "P1_template.html#stretch-questiontask-1",
    "title": "Client Report - What’s in a Name?",
    "section": "STRETCH QUESTION|TASK 1",
    "text": "STRETCH QUESTION|TASK 1\nReproduce the chart Elliot using the data from the names_year.csv file.\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here\n\n# ggplot(luke_data, aes(x = year, y = popularity)) +\n#   geom_line(color = 'blue') +\n#   ggtitle(\"Popularity of the Name 'Elliot' (Yearly Data)\") +\n#   xlab(\"Year\") +\n#   ylab(\"Popularity\") +\n#   theme_minimal()"
  },
  {
    "objectID": "P3_day2_ufo_student_starter.html",
    "href": "P3_day2_ufo_student_starter.html",
    "title": "Untitled",
    "section": "",
    "text": "#import sys\n#!{sys.executable} -m pip install requests\n# %%\n# The usuals\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n# %%\n# UFO Sightings\nurl = \"https://byuistats.github.io/DS250-Course/Skill%20Builders/json_missing.json\"\ndf = pd.read_json(url)\ndf\n\n\n\n\n\n\n\n\ncity\nshape_reported\ndistance_reported\nwere_you_abducted\nestimated_size\n\n\n\n\n0\nIthaca\nTRIANGLE\n8521.897061\nyes\n5033.900636\n\n\n1\nWillingboro\nOTHER\n-999.000000\nno\n5781.031061\n\n\n2\nHolyoke\nOVAL\n-999.000000\nno\n697203.012936\n\n\n3\nAbilene\nDISK\nNaN\nno\n5384.606775\n\n\n4\nNew York Worlds Fair\nLIGHT\n6615.781168\n-\n3417.583090\n\n\n5\nValley City\nDISK\nNaN\nno\n4280.098604\n\n\n6\nCrater Lake\nCIRCLE\n7377.893129\nno\n528289.453447\n\n\n7\nAlma\nDISK\n-999.000000\n-\n4772.748779\n\n\n8\nEklutna\nCIGAR\n5214.954126\nno\n4534.026995\n\n\n9\nHubbard\nCYLINDER\n8220.342299\n-\n4653.719175\n\n\n10\nFontana\nLIGHT\n8656.403849\nno\n4282.924400\n\n\n11\nWaterloo\nFIREBALL\nNaN\nyes\n5407.219851\n\n\n12\nBelton\nSPHERE\n9099.693894\nno\n5176.455487\n\n\n13\nKeokuk\nOVAL\nNaN\nno\n5319.530110\n\n\n14\nLudington\nDISK\n8723.814151\n-\n4744.039343\n\n\n15\nForest Home\nCIRCLE\n-999.000000\n-\n3714.042986\n\n\n16\nLos Angeles\nNone\nNaN\n-\n496646.578352\n\n\n17\nHapeville\nNone\nNaN\nno\n3659.173861\n\n\n18\nOneida\nRECTANGLE\n6197.857813\n-\n3563.000475\n\n\n19\nBering Sea\nOTHER\n7688.182878\nno\n4226.377197\n\n\n20\nNebraska\nDISK\n-999.000000\nno\n3273.803677\n\n\n23\nOwensboro\nRECTANGLE\n5911.386821\nno\n3773.207843\n\n\n24\nWilderness\nDISK\nNaN\nno\n4419.553826\n\n\n25\nSan Diego\nCIGAR\n6907.049271\nno\n780184.837408\n\n\n26\nWilderness\nDISK\nNaN\nno\n4014.634451\n\n\n27\nClovis\nDISK\nNaN\nyes\n3132.877357\n\n\n28\nLos Alamos\nDISK\n-999.000000\nno\n4030.300614\n\n\n29\nFt. Duschene\nDISK\n5432.768367\nyes\n3854.970347\n\n\n30\nSouth Kingstown\nSPHERE\n8519.036209\nno\n3476.085441\n\n\n31\nNorth Tampa\nCYLINDER\n7858.631254\nno\n4447.446627\n\n\n32\nFt. Lee\nCIGAR\n-999.000000\nno\n5899.679157\n\n\n33\nSalinas AFB\nDISK\n-999.000000\nno\n5947.348488\n\n\n34\nJasper\nFIREBALL\n6416.080266\nno\n5876.883401\n\n\n35\nWinston-Salem\nDISK\n-999.000000\nno\n5713.797947\n\n\n36\nPortsmouth\nFORMATION\nNaN\nno\n6340.368832\n\n\n37\nDallas\nSPHERE\n5601.877684\ntbd\n740725.041390\n\n\n38\nHuntington Beach\nDISK\nNaN\nno\n5095.740438\n\n\n39\nSan Antonio\nOVAL\n7862.503143\nno\n4567.720460\n\n\n40\nRoswell\nSPHERE\nNaN\nno\n5858.742486\n\n\n41\nNew York City\nDISK\n9403.433456\nyes\n4409.561573\n\n\n42\nMerced\nSPHERE\n6461.738812\n-\n4222.243212\n\n\n43\nAlice\nDISK\nNaN\nno\n6500.276196\n\n\n44\nBlairsden\nSPHERE\n6118.309085\ntbd\n4977.100109\n\n\n45\nIndex\nFIREBALL\nNaN\nno\n3334.789732\n\n\n46\nSouth Portland\nOVAL\n8409.182450\nno\n4036.546673\n\n\n47\nOak Lawn\nFORMATION\n9977.994215\nno\n3868.422555\n\n\n48\nDome\nFLASH\nNaN\ntbd\n6108.340596\n\n\n49\nConroe\nOTHER\n8941.755722\n-\n3316.854614\n\n\n50\nSyracuse\nDISK\nNaN\nno\n5391.327063\n\n\n51\nMiami\nOTHER\n5827.344837\nyes\n5172.429474\n\n\n\n\n\n\n\nWe’ll be loosely following the skill builder. Let’s start by getting to know our data and any cleaning needed. Collapse\n\ndf.shape_reported = df.shape_reported.replace(np.nan, \"missing\")\ndf.distance_reported = df.distance_reported.replace(-999, np.nan)\ndf.distance_reported = df.distance_reported.replace(np.nan, df.distance_reported.mean())\n\n\ndf2 = df.assign(\n    distance_reported2 = lambda x: x.distance_reported / 1000,\n    new_size = np.where(df.estimated_size &gt; 10000,\n                        df.estimated_size / 144,\n                        df.estimated_size)\n)\ndf2['new_size_alt'] = np.where(\n    df.city.isin(['Holyoke', 'Crater Lake', 'Los Angeles', 'San Diego', 'Dallas']), \n    df.estimated_size / 144,\n    df.estimated_size\n)\n\ncreate a table that contains some summary stats: mean and median of size for each shape.\n\ndf_spence = (\n    df2.groupby('shape_reported')\n       .agg(\n           med_size = ('new_size', 'median'),\n           mean_size = ('new_size', 'mean'),\n           how_many = ('shape_reported', 'size')\n       )\n       .reset_index()\n)\n\nlets plot the mean size by the shape\n\n(\n    ggplot(data=df_spence.sort_values('mean_size', ascending=False)) +\n    geom_bar(aes(x='shape_reported', y='mean_size'), stat='identity') +\n    labs(title='Average Size by Shape Reported', x='Shape', y='Mean Size') +\n    theme_minimal()\n)\n\n\n   \n       \n       \n       \n   \n   \n          \n   \n   \n\n\n\n\nmycategories = ['CIGAR', 'CIRCLE', 'CYLINDER', 'DISK', 'OVAL', 'RECTABGLE', 'SPERE', 'TRIANGLE', 'FIREBALL', 'FORMATION', 'LIGHT', 'OTHER', 'MISSING']\ndf_spence['shape_cat'] = pd.Categorical(df_spence['shape_reported'], categories = mycategories, ordered = True)\n\n\nggplot(data = df_spence) + geom_bar(aes(x = 'shape_cat', y = 'mean_size'), stat = 'identity')\n\n\n   \n       \n       \n       \n   \n   \n          \n   \n   \n\n\n\nlets try melting the data into longer format\n\npd.melt(df_spence, id_vars = ['shape_reported', 'how_many', 'shape_cat'], var_name = 'source', value_name = 'size')\n\n\n\n\n\n\n\n\nshape_reported\nhow_many\nshape_cat\nsource\nsize\n\n\n\n\n0\nCIGAR\n3\nCIGAR\nmed_size\n5417.950260\n\n\n1\nCIRCLE\n2\nCIRCLE\nmed_size\n3691.359873\n\n\n2\nCYLINDER\n2\nCYLINDER\nmed_size\n4550.582901\n\n\n3\nDISK\n16\nDISK\nmed_size\n4581.796584\n\n\n4\nFIREBALL\n3\nFIREBALL\nmed_size\n5407.219851\n\n\n5\nFLASH\n1\nNaN\nmed_size\n6108.340596\n\n\n6\nFORMATION\n2\nFORMATION\nmed_size\n5104.395693\n\n\n7\nLIGHT\n2\nLIGHT\nmed_size\n3850.253745\n\n\n8\nOTHER\n4\nOTHER\nmed_size\n4699.403336\n\n\n9\nOVAL\n4\nOVAL\nmed_size\n4704.704025\n\n\n10\nRECTANGLE\n2\nNaN\nmed_size\n3668.104159\n\n\n11\nSPHERE\n6\nNaN\nmed_size\n5060.512004\n\n\n12\nTRIANGLE\n1\nTRIANGLE\nmed_size\n5033.900636\n\n\n13\nmissing\n2\nNaN\nmed_size\n3554.054216\n\n\n14\nCIGAR\n3\nCIGAR\nmean_size\n5283.885471\n\n\n15\nCIRCLE\n2\nCIRCLE\nmean_size\n3691.359873\n\n\n16\nCYLINDER\n2\nCYLINDER\nmean_size\n4550.582901\n\n\n17\nDISK\n16\nDISK\nmean_size\n4685.355342\n\n\n18\nFIREBALL\n3\nFIREBALL\nmean_size\n4872.964328\n\n\n19\nFLASH\n1\nNaN\nmean_size\n6108.340596\n\n\n20\nFORMATION\n2\nFORMATION\nmean_size\n5104.395693\n\n\n21\nLIGHT\n2\nLIGHT\nmean_size\n3850.253745\n\n\n22\nOTHER\n4\nOTHER\nmean_size\n4624.173087\n\n\n23\nOVAL\n4\nOVAL\nmean_size\n4691.371208\n\n\n24\nRECTANGLE\n2\nNaN\nmean_size\n3668.104159\n\n\n25\nSPHERE\n6\nNaN\nmean_size\n4809.091772\n\n\n26\nTRIANGLE\n1\nTRIANGLE\nmean_size\n5033.900636\n\n\n27\nmissing\n2\nNaN\nmean_size\n3554.054216"
  },
  {
    "objectID": "practice.html",
    "href": "practice.html",
    "title": "untitled",
    "section": "",
    "text": "import matplotlib as plt\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\ndf = pd.read_csv(\"https://byui-cse.github.io/cse110-course/lesson11/life-expectancy.csv\")\ndf.columns = [\"Entity\", \"Code\", \"Year\", \"Life\"]\nto reference certain poieces of a dataset\n# Include and execute your code here\ndf.Entity[1]\ndf[1:3]\ndf[ :3]\ndf['Entity'] #includes just a column name\ndf['Entity'].head()\n\n0    Afghanistan\n1    Afghanistan\n2    Afghanistan\n3    Afghanistan\n4    Afghanistan\nName: Entity, dtype: object\n# Include and execute your code here\ndf.columns = ['Entity', 'Code', 'Year', 'Life']\ndf.loc[11:21, 'Year'] #Using year's column # won't work\nmy_new_data = df.loc[11:16, ['Life', 'Year']] #note the reordered columns\n# Include and execute your code here\n\ndf.iloc[20:30, [0,3]]\ndf.iloc[ :4, [0,3]]\ndf.iloc[ :4, : ]\ndf.iloc[-2: , :]\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\nLife\n\n\n\n\n19026\nZimbabwe\nZWE\n2018\n61.195\n\n\n19027\nZimbabwe\nZWE\n2019\n61.490\ndf.query('Entity== \"France\"')\ndf.query(\"Entity == 'France'\")\n#or\ndf[df['Entity'] == \"Afganistan\"]\n\ndf.query('Entity ==[\"France\", \"United States\", \"Japan\"]') #shortcut by providing a list\n\n#or\nthree_countries = df[df['Entity'] .isin([\"France\", \"United States\", \"Japan\"])]\nthree_countries['Entity'].value_counts()\n\ndf.query('Year &gt;= 200 & Year &lt;=2015')\n\n#not equals\ndf.query('Entity != \"Zimbabwe\"')\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\nLife\n\n\n\n\n0\nAfghanistan\nAFG\n1950\n27.638\n\n\n1\nAfghanistan\nAFG\n1951\n27.878\n\n\n2\nAfghanistan\nAFG\n1952\n28.361\n\n\n3\nAfghanistan\nAFG\n1953\n28.852\n\n\n4\nAfghanistan\nAFG\n1954\n29.350\n\n\n...\n...\n...\n...\n...\n\n\n18953\nZambia\nZMB\n2015\n61.737\n\n\n18954\nZambia\nZMB\n2016\n62.464\n\n\n18955\nZambia\nZMB\n2017\n63.043\n\n\n18956\nZambia\nZMB\n2018\n63.510\n\n\n18957\nZambia\nZMB\n2019\n63.886\n\n\n\n\n18958 rows × 4 columns\ntop5_mean = df.sort_values([\"Year\", \"Life\"], ascending = False).head(5).Life.mean()\ndf.Entity.value_counts()\ndf.Entity.unique().shape[0] ## of unique countries in the dataset\n\n243\n#usa = df.query('Entity == \"United States\"')\nusa = df.query('Entity == \"United States\"')\n(\n    ggplot(data = usa,\n            mapping = aes(x = 'Life')) +\n            geom_histogram()\n)"
  },
  {
    "objectID": "practice.html#day-2",
    "href": "practice.html#day-2",
    "title": "untitled",
    "section": "Day 2",
    "text": "Day 2\n\nCreat the usa dataset\n\nusa = df.query('Entity == \"United States\"')\n(\n    ggplot(data = usa,\n            mapping = aes(x = 'Life')) +\n            geom_histogram()\n)"
  },
  {
    "objectID": "practice.html#task-1-usa-life-expectancy",
    "href": "practice.html#task-1-usa-life-expectancy",
    "title": "untitled",
    "section": "Task 1 USA life expectancy",
    "text": "Task 1 USA life expectancy\n\nusa2 = usa.query('Year &gt; 1920 & Year &lt;=2000')\n(\n    ggplot(data = usa2, \n            mapping = aes(x = \"Year\", y = \"Life\")) +\n            geom_line()\n)\n\n\n   \n       \n       \n       \n   \n   \n          \n   \n   \n\n\n\nOr you can change\n\np= (\n    ggplot(data = usa, \n    mapping = aes(x= 'Year', y = 'Life')) + \n    geom_line() + \n    geom_rect(xmin = 1918, xmax = 1920, ymin = 46, ymax = 55, fill = \"red\", alpha = .2) + \n    geom_vline(xintercept = 1943, color = \"red\", linetype = 2) +\n    geom_text(label = \"Last decrease in 1943\", x = 1947, y = 55, angel = 90) +\n    labs(y = \"Life Expectancy (yrs)\", title = \"United States\") +\n    theme_classic() + \n    theme(plot_title= element_text(hjust = .5, size = 38, color= \"blue\" ), \n    panel_grid = element_blank())\n\n    \n\n)"
  },
  {
    "objectID": "practice.html#practice-marketing",
    "href": "practice.html#practice-marketing",
    "title": "untitled",
    "section": "practice marketing",
    "text": "practice marketing\n\np + theme(axis_title_x = element_blank())\n\n\n   \n       \n       \n       \n   \n   \n          \n   \n   \n\n\n\n\ngraphing more than one —–\n1 line per country\n\nthree = df.query('Entity == [\"United States\", \"Japan\", \"France\"]')\n(\n    ggplot(data = three, \n    mapping = aes(x = 'Year', y = 'Life', color = 'Entity')) + \n    geom_line()\n)"
  },
  {
    "objectID": "P3_template.html",
    "href": "P3_template.html",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Show the code\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\nShow the code\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\ndf = pd.read_json(\"https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json\")"
  },
  {
    "objectID": "P3_template.html#elevator-pitch",
    "href": "P3_template.html#elevator-pitch",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nA SHORT (2-3 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS. (Note: this is not a summary of the project, but a summary of the results.)\nA Client has requested this analysis and this is your one shot of what you would say to your boss in a 2 min elevator ride before he takes your report and hands it to the client."
  },
  {
    "objectID": "P3_template.html#questiontask-1",
    "href": "P3_template.html#questiontask-1",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.__\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here\ndf = pd.DataFrame({\"A\": [1, np.nan, 3], \"B\": [4, 5, np.nan]})\nprint(df)\n(\"https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json\")\n\nmissing_values = df.isna().sum()\nmissing_23_values = missing_values[missing_values == 23]\n\nprint(missing_23_values)\n\n\n     A    B\n0  1.0  4.0\n1  NaN  5.0\n2  3.0  NaN\nSeries([], dtype: int64)"
  },
  {
    "objectID": "P3_template.html#for-quiz",
    "href": "P3_template.html#for-quiz",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "for quiz:",
    "text": "for quiz:\n\n\nShow the code\ndf = pd.read_json(\"https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json\")\n\nn_a_count = (df['month'] == 'n/a').sum()\n\nprint(f\"Number of records with 'n/a' in the 'month' column: {n_a_count}\")\n\n\nNumber of records with 'n/a' in the 'month' column: 27"
  },
  {
    "objectID": "P3_template.html#questiontask-2",
    "href": "P3_template.html#questiontask-2",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays? Describe the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here"
  },
  {
    "objectID": "P3_template.html#questiontask-3",
    "href": "P3_template.html#questiontask-3",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length? Describe the metric you chose and why you chose it to calculate your answer. Include one chart to help support your answer, with the x-axis ordered by month. (To answer this question, you will need to remove any rows that are missing the Month variable.)\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here"
  },
  {
    "objectID": "P3_template.html#questiontask-4",
    "href": "P3_template.html#questiontask-4",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). You will need to replace all the missing values in the Late Aircraft variable with the mean. Show your work by printing the first 5 rows of data in a table. Use these three rules for your calculations:\na. 100% of delayed flights in the Weather category are due to weather  \na. 30% of all delayed flights in the Late-Arriving category are due to weather  \na. From April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%    \ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here\n\n\n\n\nShow the code\n# Include and execute your code here\n\n\n\n\nShow the code\n# Include and execute your code here"
  },
  {
    "objectID": "P3_template.html#questiontask-5",
    "href": "P3_template.html#questiontask-5",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Describe what you learn from this graph.\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here"
  },
  {
    "objectID": "P3_template.html#stretch-questiontask-1",
    "href": "P3_template.html#stretch-questiontask-1",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "STRETCH QUESTION|TASK 1",
    "text": "STRETCH QUESTION|TASK 1\nWhich delay is the worst delay? Create a similar analysis as above for Weahter Delay with: Carrier Delay and Security Delay. Compare the proportion of delay for each of the three categories in a Chart and a Table. Describe your results.\ntype your results and analysis here\n\n\nShow the code\n# Include and execute your code here"
  }
]